
# Regulatory & Policy Copilot â€” Agentic RAG (Multi-hop, Section-Cited)

An agentic RAG that answers complex compliance questions from regulatory PDFs/HTML with a **planner â†’ retriever â†’ answerer â†’ critic â†’ repair** loop, **hybrid retrieval (BM25 + dense + cross-encoder rerank)**, and **inline section-level citations** like `[privacy_act_2024 Â§3.1]`.


## ğŸš€ Quickstart (uv + Groq)

```bash
# 0) Clone and enter the repo
# git clone https://github.com/yourname/agentic-rag-regcopilot && cd agentic-rag-regcopilot

# 1) Create & activate env
uv venv .venv && source .venv/bin/activate   # Windows: .\.venv\Scripts\activate

# 2) Install deps
uv sync

# 3) Configure API key
cp .env.example .env           # Windows: copy .env.example .env
# edit .env and set: GROQ_API_KEY=your_key_here

# 4) (Optional) Add sample docs
# Unzip regulatory_sample_docs.zip into data/sample/ or use your own PDFs/HTML

# 5) Ingest docs (build vectorstore + embeddings)
uv run -m app.ingest.ingest --input_dir data/sample --db_path .vectorstore

# 6) Run API
uv run uvicorn app.api:app --reload --port 8000
````

**Try a query**

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "Is cross-border transfer allowed without explicit consent?", "top_k": 8}'
```

**Optional UI**

```bash
uv run streamlit run app/ui.py
```

---

## ğŸ§  What makes it â€œagenticâ€

* **Planner** decomposes the user question into verifiable sub-questions.
* **Hybrid Retrieval**: BM25 + sentence-transformer embeddings â†’ RRF fusion â†’ **bge-reranker** cross-encoder.
* **Answerer** writes ONLY from retrieved evidence with **inline citations**.
* **Critic/Judge** verifies every factual sentence has a citation; on failure, **Repair** suggests new sub-queries and retries.
* **Abstain** behavior for insufficient evidence.

---

## ğŸ§± Project Structure

```
app/
  agents/
    planner.py        # make sub-questions
    retriever.py      # collect top-k evidence per subq
    answerer.py       # grounded sub-answers + merge
    critic.py         # check citations/consistency
    repair.py         # propose new queries if fail
  ingest/
    loaders.py        # PDF/HTML loaders + chunking
    ingest.py         # build corpus.jsonl + embeddings
  retrievers/
    hybrid_rrf.py     # BM25 + dense + cross-encoder rerank
  api.py              # FastAPI endpoint /ask
  config.py           # env + model names
  graph.py            # runtime wiring + agentic loop
  models.py           # Groq LLM, embeddings, reranker
  typing.py           # shared types
  ui.py               # tiny Streamlit UI (optional)
data/
  sample/             # put your PDFs/HTML here
.vectorstore/         # generated on ingest (corpus + embeddings)
pyproject.toml        # uv project config
.env.example          # set GROQ_API_KEY
```

---

## âš™ï¸ Configuration

**Environment (.env)**

```
GROQ_API_KEY=your_groq_key
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
RERANK_MODEL=BAAI/bge-reranker-base
DB_PATH=.vectorstore
```

**Models**

* LLM: `llama-3.3-70b-versatile` via **Groq**
* Embeddings: `sentence-transformers/all-MiniLM-L6-v2` (swap to `bge-m3` for higher recall)
* Reranker: `BAAI/bge-reranker-base` (cross-encoder)

---

## ğŸ“¥ Add Documents

Drop your **regulatory PDFs/HTML** into `data/sample/` (e.g., acts, guidelines, regulator FAQs).

Sample synthetic docs included:

* `privacy_act_2024.html`
* `cross_border_transfer_code_2023.html`
* `regulator_faq_consent.html`
* `payments_kyc_rules_2022.html`
* `dpia_guideline_2023.html`

Re-ingest whenever you add/update content:

```bash
uv run -m app.ingest.ingest --input_dir data/sample --db_path .vectorstore
```

---

## ğŸ§ª Example Questions

* â€œIs cross-border transfer allowed without explicit consent?â€
  â†’ cites `Â§3.1` / safeguards & consent requirements.

* â€œHow long must we keep KYC records after off-boarding?â€
  â†’ cites KYC retention section.

* â€œDo we need a DPIA for large-scale cross-border analytics?â€
  â†’ cites DPIA triggers + minimum contents.

---

## ğŸ“Š Evaluation (hooks)

A placeholder `evals/ragas_suite.py` is included. Suggested metrics:

* **RAGAS**: faithfulness, answer relevancy, context precision/recall
* **Latency & Cost**: p50/p95 + token usage
* **Drift checks**: re-run evals after each crawl/re-ingest

Add a small gold set under `data/evals/` to compare:

* **Baseline RAG** vs **Agentic (with Critic/Repair)**

---

## ğŸ› ï¸ Troubleshooting

* **`ZeroDivisionError` in BM25** â†’ ingest ran on an empty folder. Ensure `data/sample/` has files, then re-ingest.
* **`.env not loaded`** â†’ we `load_dotenv()` in `app/config.py`. Verify `.env` exists and the key name is `GROQ_API_KEY`.
* **Large download for reranker (1.1GB)** â†’ first run is slow; it caches under `~/.cache/huggingface/`.
* **Windows symlink warning from HF Hub** â†’ optional; enable Developer Mode or ignore the warning.
* **Slow answers** â†’ reduce `top_k`, switch to a lighter reranker, or use smaller embedding model.

---

## ğŸ§© Roadmap Ideas

* Freshness guard (min source date per answer)
* Exact span highlighting in source viewer
* Multi-tenant doc spaces + access control
* Periodic crawler + re-embed deltas
* Automated evals in CI (TruLens/RAGAS + thresholds)

---

## ğŸ“œ License

MIT 

---

